{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch using  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('pytorch using ', device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "max_words = 10000\n",
    "sentence_len = 20\n",
    "pred_len = 1  # fix\n",
    "train_len = sentence_len - pred_len\n",
    "\n",
    "max_samples = int(4e5) #6e6 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre trained word embeddings\n",
    "\n",
    "import bcolz\n",
    "\n",
    "glove_path = './'\n",
    "vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "# if files do not exist run glove_embedding.ipynb first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(words, max_words=None):\n",
    "    if max_words is None:\n",
    "        return list(set(words))\n",
    "    \n",
    "    vocab = {}\n",
    "    for word in word_seq:\n",
    "        try:\n",
    "            vocab[word] += 1\n",
    "        except KeyError:\n",
    "            vocab[word] = 1\n",
    "    most_freq = sorted(vocab.keys(), key=vocab.get, reverse=True)\n",
    "    return most_freq[:max_words]\n",
    "\n",
    "\n",
    "def replace_by_tbl(text, tbl):\n",
    "    for k,v in tbl.items():\n",
    "        text = text.replace(k,v)\n",
    "    return text\n",
    "\n",
    "\n",
    "# preprossing mapping tables\n",
    "apostrophe_tbl = {ord(key): '\\'' for key in '\\`\\´\\’'}\n",
    "shortform_tbl = {\n",
    "    'n\\'t' : ' not',\n",
    "    '\\'ve' : ' have',\n",
    "    '\\'ll' : ' will',\n",
    "    '\\'m' : ' am',\n",
    "    '\\'re' : ' are',\n",
    "    '\\'s' : ' is',\n",
    "    '\\'d' : ' would',\n",
    "}\n",
    "remove_apostrophe_tbl = {ord('\\''): None}\n",
    "restore_oclock = {'oclock': 'o\\'clock'}\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(apostrophe_tbl)       # uniform apostrophe\n",
    "    text = replace_by_tbl(text, shortform_tbl)  # split shortforms\n",
    "    text = text.translate(remove_apostrophe_tbl)  # remove rest of '\n",
    "    text = replace_by_tbl(text, restore_oclock)   # restore o'clock\n",
    "    return text\n",
    "\n",
    "\n",
    "def text2words(text, vocab_set=None):\n",
    "    words = text_to_word_sequence(preprocess_text(text))\n",
    "    words = [ word for word in words if word in word2idx]\n",
    "    if not vocab_set is None:\n",
    "        words = [ word for word in words if word in vocab_set]\n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocablary: 10000\n"
     ]
    }
   ],
   "source": [
    "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
    "    texts = [line.strip('\\n') for line in file]\n",
    "\n",
    "# sequences of words\n",
    "word_seqs = [text2words(text) for text in texts]\n",
    "\n",
    "# flatten seqneces to one long sequence \n",
    "word_seq = [inner for outer in word_seqs for inner in outer]\n",
    "\n",
    "# vocabulary - list of words that are used\n",
    "vocab = build_vocab(word_seq, max_words)\n",
    "vocab_size = len(vocab)\n",
    "print(f'size of vocablary: {vocab_size}')\n",
    "\n",
    "# remove all words that are not in the vocabulary\n",
    "vocab_set = set(vocab)\n",
    "word_seq = [word for word in word_seq if word in vocab_set]\n",
    "\n",
    "# tokenized sequence of words\n",
    "w2tk = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    w2tk[word] = i\n",
    "\n",
    "tkn_seq = [w2tk[word] for word in word_seq]\n",
    "tkn_seq = np.array(tkn_seq)\n",
    "\n",
    "def tk2emb(token):\n",
    "    return glove[vocab[token]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available samples:  5070539\n",
      "training samples:  400000\n"
     ]
    }
   ],
   "source": [
    "# Sliding window to generate train data\n",
    "seq = []\n",
    "for i in range(len(tkn_seq)-sentence_len):\n",
    "    seq.append(tkn_seq[i:i+sentence_len])\n",
    "\n",
    "# reduce length of seq for performance reasons and of cause test_set\n",
    "print('available samples: ', len(seq))\n",
    "seq = seq[:max_samples]\n",
    "\n",
    "# set data, label\n",
    "X = []\n",
    "y = []\n",
    "for i in seq:\n",
    "    X.append(i[:train_len])\n",
    "    y.append(i[-1])\n",
    "\n",
    "num_samples = len(y)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print('training samples: ', num_samples)\n",
    "\n",
    "# split data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.4)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weights for the embedding layer\n",
    "\n",
    "weights_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "\n",
    "print(weights_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_2 = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./model_2_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "x = np.asarray(trainX)\n",
    "y = np.asarray(trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.fit(x,y, epochs = 10, batch_size = 512, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, sentence, iterations=10):\n",
    "\n",
    "    seq = [w2tk[word] for word in text2words(sentence, vocab_set)]\n",
    "    #if(len(seq) < train_len): return -1\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        iput = np.array(seq[-19:])\n",
    "        iput = np.vstack(iput).T\n",
    "        iput = torch.from_numpy(iput).type(X_type)\n",
    "\n",
    "        # oput = F.log_softmax( model(iput) )  # prob dist\n",
    "        oput = model(iput)\n",
    "        \n",
    "        oput = oput.cpu().detach().numpy()[0]\n",
    "        oput = np.argmax(oput) # max of softmax to long\n",
    "        seq.append(oput)\n",
    "\n",
    "    pred_sen = ''\n",
    "    for tk in seq:\n",
    "        pred_sen += vocab[tk] + ' '\n",
    "    return pred_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"Good evening today i will talk about something of high importance. I hope you will enjoy hearing about\"\n",
    "print(gen(model_2, sen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
