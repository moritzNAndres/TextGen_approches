{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import time\n",
    "import pickle\n",
    "import bcolz\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "%load_ext tensorboard\n",
    "\n",
    "logdir = './logs/'\n",
    "modeldir = './models/'\n",
    "rundir = './run/'\n",
    "glove_path = './glove/'\n",
    "\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_words = 10000\n",
    "sentence_len = 20\n",
    "pred_len = 1  # fix\n",
    "train_len = sentence_len - pred_len\n",
    "\n",
    "batch_size = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load glove \n",
    "(if files do not exist run glove_embedding.ipynb first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre trained word embeddings\n",
    "\n",
    "vectors = bcolz.open(f'{glove_path}6B.50.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocabulary and text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(words, max_words=None):\n",
    "    if max_words is None:\n",
    "        return list(set(words))\n",
    "    \n",
    "    vocab = {}\n",
    "    for word in word_seq:\n",
    "        try:\n",
    "            vocab[word] += 1\n",
    "        except KeyError:\n",
    "            vocab[word] = 1\n",
    "    most_freq = sorted(vocab.keys(), key=vocab.get, reverse=True)\n",
    "    return most_freq[:max_words]\n",
    "\n",
    "\n",
    "def replace_by_tbl(text, tbl):\n",
    "    for k,v in tbl.items():\n",
    "        text = text.replace(k,v)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate({                 # uniform apostrophe\n",
    "        ord(key): '\\'' for key in '\\`\\´\\’'\n",
    "    })\n",
    "    text = replace_by_tbl(text, {           # split shortforms\n",
    "        'n\\'t' : ' not',\n",
    "        '\\'ve' : ' have',\n",
    "        '\\'ll' : ' will',\n",
    "        '\\'m' : ' am',\n",
    "        '\\'re' : ' are',\n",
    "        '\\'s' : ' is',\n",
    "        '\\'d' : ' would',\n",
    "    })\n",
    "    text = text.translate({                 # remove rest of '\n",
    "        ord('\\''): None\n",
    "    })\n",
    "    text = replace_by_tbl(text, {           # restore o'clock\n",
    "        'oclock': 'o\\'clock'\n",
    "    })\n",
    "    return text\n",
    "\n",
    "\n",
    "def text2words(text, vocab_set=None):\n",
    "    words = text_to_word_sequence(preprocess_text(text))\n",
    "    words = [ word for word in words if word in word2idx]\n",
    "    if not vocab_set is None:\n",
    "        words = [ word for word in words if word in vocab_set]\n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size of vocablary: 10000\n"
    }
   ],
   "source": [
    "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
    "    texts = [line.strip('\\n') for line in file]\n",
    "\n",
    "# sequences of words\n",
    "word_seqs = [text2words(text) for text in texts]\n",
    "\n",
    "# flatten seqneces to one long sequence \n",
    "word_seq = [inner for outer in word_seqs for inner in outer]\n",
    "\n",
    "# vocabulary - list of words that are used\n",
    "vocab = build_vocab(word_seq, max_words)\n",
    "vocab_set = set(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# remove all words that are not in the vocabulary\n",
    "word_seq = [word for word in word_seq if word in vocab_set]\n",
    "\n",
    "# tokenized sequence of words\n",
    "w2tk = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    w2tk[word] = i\n",
    "\n",
    "# create token sequence\n",
    "tkn_seq = [w2tk[word] for word in word_seq]\n",
    "tkn_seq = np.array(tkn_seq)\n",
    "\n",
    "\n",
    "def tk2emb(token):\n",
    "    return glove[vocab[token]]\n",
    "\n",
    "print(f'size of vocablary: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sliding window to generate train data\n",
    "seq = []\n",
    "for i in range(len(tkn_seq)-sentence_len):\n",
    "    seq.append(tkn_seq[i:i+sentence_len])\n",
    "\n",
    "# set data, label\n",
    "X = []\n",
    "y = []\n",
    "for i in seq:\n",
    "    X.append(i[:train_len])\n",
    "    y.append(i[-1])\n",
    "\n",
    "num_samples = len(y)\n",
    "print('training samples: ', num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# split data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.4)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=.5)\n",
    "\n",
    "# make a tf dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices( (X_train, y_train) ).shuffle(buffer_size=1000).batch(batch_size)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices( (X_valid, y_valid) ).shuffle(buffer_size=1000).batch(batch_size)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices( (X_test , y_test ) ).shuffle(buffer_size=1000).batch(batch_size)\n",
    "\n",
    "# use prefetching to parallize training and loading of next batch\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_ds  =  test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# cache dataset on first epoch\n",
    "train_ds = train_ds.cache()\n",
    "valid_ds = valid_ds.cache()\n",
    "test_ds  =  test_ds.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the weights for the glove embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "\n",
    "print(weights_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(121)   \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])   \n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Embedding(input_dim=vocab_size, \n",
    "            output_dim=embedding_dim, \n",
    "            input_length=train_len, \n",
    "            trainable=False, \n",
    "            weights=[weights_matrix]),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = ''.join(['_'+str(t) for t in time.localtime(time.time())[:4]])\n",
    "filepath = modeldir + 'model_2' + date_str +'.hdf5'\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "checkpoint2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint1, checkpoint2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2_history = model_2.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs = 30, batch_size = 1024, callbacks = callbacks_list)\n",
    "plot_hist(m2_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_2.evaluate(X_test, y_test, batch_size=1024)\n",
    "print(f'The model achived a loss of {test_loss:.4f} and a accuracy of {test_accuracy * 100:.2f}% on the test-set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, sentence, max_len=30):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = [w2tk[word] for word in text2words(sentence, vocab_set)]\n",
    "    max_len = max_len+len(tokenized_sent)\n",
    "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
    "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
    "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
    "    while len(tokenized_sent) < max_len:\n",
    "        padded_sentence = tf.keras.preprocessing.sequence.pad_sequences([tokenized_sent][-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent.append(op.argmax() )\n",
    "        \n",
    "    return \" \".join([vocab[tk] for tk in tokenized_sent]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test start-sentences\n",
    "\n",
    "sen1 = \"Good evening everyone. My name\"\n",
    "sen2 = \"Donald Trump\"\n",
    "sen3 = \"Climate change\"\n",
    "sen4 = \"My research about\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "good evening everyone my name is the first thing is the first time i am going to show you a little bit of the other things i am going to show you a little bit\n\ndonald trump is the first thing is the first thing is the first thing is the first thing is the first time i am going to show you a little bit of\n\nclimate change is the same thing that is the same thing that is the same thing that is the same thing that is the same thing that is the same thing that\n\nmy research about the world is the first thing that is the first thing that is the first thing that is the same thing that is the same thing that is the same\n\n"
    }
   ],
   "source": [
    "\n",
    "model_name = 'model_2_2020_7_23_21'\n",
    "\n",
    "model = tf.keras.models.load_model(modeldir + model_name + '.hdf5')\n",
    "\n",
    "print(gen(model, sen1))\n",
    "print(gen(model, sen2))\n",
    "print(gen(model, sen3))\n",
    "print(gen(model, sen4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow-envi': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595773544969"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}