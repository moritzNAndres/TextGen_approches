{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Text Generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voqx3u6UPj4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80d0ac35-448c-49c7-c8b3-2aed3f7c0799"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import randint, choice\n",
        "from keras import backend as K\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBg6L102P_cP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbd72dfd-efe2-44a9-b88b-7c3994e2c676"
      },
      "source": [
        "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
        "    train_data = [line.strip('\\n') for line in file]\n",
        "\n",
        "print('Number of training sentences: ', len(train_data))\n",
        "\n",
        "max_words = 50000 # Max size of the dictionary\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "sequences = tokenizer.texts_to_sequences(train_data)\n",
        "\n",
        "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
        "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
        "text = [item for sublist in sequences for item in sublist]\n",
        "vocab_size = len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences:  2457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmhvlq1kSBe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d1e613a-a1d8-4285-c328-941e2159a2c5"
      },
      "source": [
        "# Training on 19 words to predict the 20th\n",
        "sentence_len = 20\n",
        "pred_len = 1\n",
        "train_len = sentence_len - pred_len\n",
        "seq = []\n",
        "# Sliding window to generate train data\n",
        "for i in range(len(text)-sentence_len):\n",
        "    seq.append(text[i:i+sentence_len])\n",
        "# Reverse dictionary to decode tokenized sequences back to words\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
        "trainX = []\n",
        "trainy = []\n",
        "for i in seq:\n",
        "    trainX.append(i[:train_len])\n",
        "    trainy.append(i[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5117293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMoZ8-dsR-CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(embdding_size=50, LSTM_sizes=[100,100], hidden_layer={'size': 100, 'act':'relu'}, dropout_rate=0.1):\n",
        "    return Sequential([\n",
        "        Embedding(vocab_size+1, embdding_size, input_length=train_len),\n",
        "        LSTM(LSTM_sizes[0], return_sequences=True),\n",
        "        LSTM(LSTM_sizes[1]),\n",
        "        Dense(hidden_layer['size'], activation=hidden_layer['act']),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_FWCXHyHVju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(x,y,setting, EPOCHS, validation_data, callbacks):\n",
        "\n",
        "    # build model\n",
        "    model = build_model(embdding_size=setting['embedding_size'], LSTM_sizes=setting['LSTM_sizes'], hidden_layer=setting['hidden_layer'], dropout_rate=setting['dropout_rate'])\n",
        "    model.compile(optimizer=setting['optimizer'], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    K.set_value(model.optimizer.learning_rate, setting['learning_rate'])\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x, y, epochs=EPOCHS, batch_size = setting['BATCH_SIZE'], validation_data = validation_data, callbacks=callbacks)\n",
        "    \n",
        "    return history.history['loss'][EPOCHS-1], model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJymdy2iaLPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_search_hyperparameters(num_tries=10):\n",
        "    losses = np.zeros(num_tries)\n",
        "    settings = []\n",
        "    x = np.asarray(trainX)\n",
        "    y = np.asarray(trainy)\n",
        "\n",
        "    for i in range(num_tries):\n",
        "        LSTM_sizes = randint(50, 200, 2)\n",
        "        embedding_size = randint(50, 200)\n",
        "        BATCH_SIZE = randint(300,2000)\n",
        "        hidden_layer_size = randint(50,200)\n",
        "        hidden_layer_activation = np.random.choice(['relu', 'elu'])\n",
        "        optimizer = np.random.choice(['adam', 'nadam', 'rmsprop'])\n",
        "        hidden_layer = {'size': hidden_layer_size, 'act': hidden_layer_activation}\n",
        "        dropout_rate = np.random.uniform(0.1, 0.5)\n",
        "        EPOCHS = 1\n",
        "        learning_rate = np.random.uniform(0.01, 0.0001)\n",
        "\n",
        "        \n",
        "        setting = {'LSTM_sizes': LSTM_sizes, 'embedding_size': embedding_size, 'BATCH_SIZE': BATCH_SIZE, 'hidden_layer': hidden_layer, 'optimizer': optimizer, 'learning_rate': learning_rate, 'dropout_rate': dropout_rate}\n",
        "        print(setting)\n",
        "        settings.append(setting)\n",
        "        losses[i] = run_model(x, y, setting, EPOCHS)\n",
        "    \n",
        "    return losses, settings\n",
        "\n",
        "losses, settings = random_search_hyperparameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgLMDG4PIvvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Valdiating 4 best model settings regarding validation loss and accuracy\n",
        "l = len(trainy)\n",
        "c = math.floor(l*0.75)\n",
        "np.random.seed(42)\n",
        "perm = np.random.permutation(l)\n",
        "trainX = np.asarray(trainX)[perm]\n",
        "trainy = np.asarray(trainy)[perm]\n",
        "x = trainX[:c]\n",
        "y = trainy[:c]\n",
        "val_x = trainX[c:]\n",
        "val_y = trainy[c:]\n",
        "settings = [{'LSTM_sizes': np.array([151, 177]), 'embedding_size': 143, 'BATCH_SIZE': 972, 'hidden_layer': {'size': 144, 'act': 'elu'}, 'optimizer': 'nadam', 'learning_rate': 0.004159842835701155, 'dropout_rate': 0.1},\n",
        "            {'LSTM_sizes': np.array([154, 193]), 'embedding_size': 66, 'BATCH_SIZE': 770, 'hidden_layer': {'size': 129, 'act': 'relu'}, 'optimizer': 'adam', 'learning_rate': 0.006717827078582077, 'dropout_rate': 0.26103763276759384},\n",
        "            {'LSTM_sizes': np.array([196, 180]), 'embedding_size': 192, 'BATCH_SIZE': 1247, 'hidden_layer': {'size': 187, 'act': 'elu'}, 'optimizer': 'adam', 'learning_rate': 0.0028276114501194676, 'dropout_rate': 0.3828645817685946},\n",
        "            {'LSTM_sizes': np.array([196, 110]), 'embedding_size': 130, 'BATCH_SIZE': 1408, 'hidden_layer': {'size': 193, 'act': 'sigmoid'}, 'optimizer': 'rmsprop', 'learning_rate': 0.00617748089357869, 'dropout_rate': 0.1}]\n",
        "\n",
        "# Checkpoints for storing the best models\n",
        "checkpoint1 = ModelCheckpoint('model_best_val_loss_weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "checkpoint2 = ModelCheckpoint('model_best_val_acc_weights.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1, checkpoint2]\n",
        "\n",
        "# The best model is using the first setting\n",
        "loss, best_model = run_model(x, y, settings[0], 0, (val_x, val_y), callbacks_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsAjMegLCaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model.load_weights('model_best_val_loss_weights.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqJw2aqR7mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First text generation method\n",
        "# Keeps track of the k most likly sentences\n",
        "\n",
        "def gen(model,seq,max_len = 10, k=5):\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])    \n",
        "    tokenized_sent =  [(tokenized_sent[0], [1])]\n",
        "\n",
        "    while len(tokenized_sent[0][0]) < max_len:\n",
        "        new_sentences = []\n",
        "        for entry in tokenized_sent:\n",
        "            sent = entry[0]\n",
        "            padded_sentence = tf.keras.preprocessing.sequence.pad_sequences([sent][-19:],maxlen=19)\n",
        "            op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "            op = np.array(op).reshape((-1))\n",
        "            # get k best options for the next word\n",
        "            top_k_words = op.argsort()[-k:]\n",
        "            for i in top_k_words:\n",
        "                s = sent.copy()\n",
        "                s.append(i)\n",
        "                p = entry[1]\n",
        "                p.append(op[i])\n",
        "                # add tuple of (sentence string \"s\" and the probability \"p\") to list of possible new sentences\n",
        "                new_sentences.append((s, p))\n",
        "\n",
        "        # compute probability for sentences\n",
        "        ps = np.array([np.prod(s[1]) for s in new_sentences])\n",
        "        # select k best sentences\n",
        "        top_k_sent = ps.argsort()[-k:]\n",
        "        tokenized_sent = np.array(new_sentences)[top_k_sent]\n",
        "    return [\" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[i][0])) for i in range(k)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYe4ST02daPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(*gen(best_model,'Good evening everyone. My name is'), sep='\\n')\n",
        "print(*gen(best_model,'Donald Trump'), sep='\\n')\n",
        "print(*gen(best_model,'The climate change'), sep='\\n')\n",
        "print(*gen(best_model,'My research about'), sep='\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0AUQOLQ0Gj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second method for text generation\n",
        "# generate n texts using the output probabilties of the model\n",
        "# t is \"anti-temperature\" : high value -> use most likely words\n",
        "#                           low value -> use also some unlikely word\n",
        "\n",
        "def generate(model,seq,max_len = 30, n=5, t=2):\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    \n",
        "    tokenized_sent =  [tokenized_sent[0] for _ in range(n)]\n",
        "\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        new_sentences = []\n",
        "        for i, sent in enumerate(tokenized_sent):\n",
        "            padded_sentence = tf.keras.preprocessing.sequence.pad_sequences([sent][-19:],maxlen=19)\n",
        "            op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "            op = np.array(op).reshape((-1))\n",
        "            op = np.power(op, t)\n",
        "            op = op/op.sum()\n",
        "            w = np.random.choice(range(vocab_size), p = op)\n",
        "            s = sent.copy()\n",
        "            s.append(w)\n",
        "            tokenized_sent[i] = s\n",
        "\n",
        "    return [\" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[i])) for i in range(k)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6BTjZLe1VYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "b4673fb3-1675-4393-a247-85da5650b595"
      },
      "source": [
        "print(*generate(best_model,'Good evening everyone. My name is'), sep='\\n')\n",
        "print(*generate(best_model,'Donald Trump'), sep='\\n')\n",
        "print(*generate(best_model,'The climate change'), sep='\\n')\n",
        "print(*generate(best_model,'My research about'), sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good evening everyone my name is a punk and i am an expert in my life and i have to be a little bit of a sort of feeling of reality and i couldn't tell you\n",
            "good evening everyone my name is i am a blogger who was a young woman who was the most famous woman and i was a girl and she was a little girl and she was a\n",
            "good evening everyone my name is an english composer i am a boy i can imagine that i had to be a professor i was a teenager i was a musician and i was an activist\n",
            "good evening everyone my name is a woman who is a friend who is the father of the world who and the guy who is in a way and so what does this mean is that\n",
            "good evening everyone my name is so wonderful i don't know what to look like i can say that i can look at the world and i can show you it is that the way i\n",
            "donald trump refers to his phone and his hair and he was a professor he was a painter he was a little girl he was an actor he had a little bit\n",
            "donald trump i love a beautiful bicycle and i grew up in a restaurant in my hometown of england and i was arrested for this story and i had a very time\n",
            "donald trump and he's a mother in the countryside and he's a big older man for the day he said i know i know i don't want to cut down or take\n",
            "donald trump who are very primitive i can see the same video of the museum of the 19th century and i think we have to do the same thing to make the\n",
            "donald trump and his father and his life and his life and i was the most profound and powerful part of my life i thought i had to write about everything i\n",
            "the climate change is a very important story in a particular way and that was a bit of a story and i was looking to show you this is a picture of an\n",
            "the climate change is a full car for the most part of the world in the world in the middle east but also in the world of the world and i'm going to\n",
            "the climate change in his life and the work that he would ever meet and he came up with a series of things that were going to be the kind of thing that\n",
            "the climate change in the city of virginia in the south bronx and the world is going to have to go back to the united nations and the last time i can tell\n",
            "the climate change and an art of the art of the world in the world and i grew up in the world i was in a war and i started to know what\n",
            "my research about a single black girl and the father of the army was dressed in the trash and he was just a little boy he was a miner he was a man\n",
            "my research about a day and i was a writer i was a great woman i was a teacher i loved to understand me and i had to work with the world and\n",
            "my research about a piece of music and the same thing's going to be a little bit of a moment to go into the middle east and i think this is a very\n",
            "my research about a new vocabulary of a new story of character and the beauty of a new kind of sculpture that i see i can see your light and music and i\n",
            "my research about the day i was in size and i was a little boy and i loved it i had a lot of money i was going to get my parents to\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}