{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TED talk generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIzR7_iahEJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40378176-cf2d-47ae-a5d9-dc2f33df4022"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import os\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBQzwf4khR4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = '/content/processed_texts.csv'\n",
        "# Read and decode file\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lP7k8TFhV6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCY0QhHFhk13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE6lJgdxhrt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWX9Vz9gh_l2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "#dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtMBc9yKiAql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qQRKb6miCmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim = 256, rnn_units = 1024, batch_size = 64, hidden_layer = None, softmax=False):\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform')])\n",
        "     \n",
        "    if hidden_layer is not None:\n",
        "        model.add(tf.keras.layers.Dense(hidden_layer['size'], activation=hidden_layer['act']))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(vocab_size, activation=('softmax' if softmax else None)))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9Cjob1iiFbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"original_gru_model.png\",\n",
        "    show_shapes=True,\n",
        "    show_layer_names=False,\n",
        "    rankdir=\"TB\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dig5NoLLiWIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6_06N_vKJcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(dataset, setting, EPOCHS, p = 0.8, callbacks=[]):\n",
        "    x = dataset.shuffle(BUFFER_SIZE)\n",
        "\n",
        "    # Split dataset in training and validation data\n",
        "    if p >= 0 and p <= 1:\n",
        "        l = len(list(dataset))\n",
        "        c = math.floor(l*p)\n",
        "        validation_data = x.skip(c).batch(setting['BATCH_SIZE'], drop_remainder=True)\n",
        "        x = x.take(c).batch(setting['BATCH_SIZE'], drop_remainder=True)\n",
        "\n",
        "    #build model\n",
        "    model = build_model(vocab_size = len(vocab), embedding_dim=setting['embedding_dim'], rnn_units=setting['rnn_units'], batch_size=setting['BATCH_SIZE'], hidden_layer = setting['hidden_layer'], softmax=setting['softmax'])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    K.set_value(model.optimizer.learning_rate, setting['learning_rate'])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(x, epochs=EPOCHS, validation_data = validation_data, callbacks = callbacks)\n",
        "\n",
        "    return history.history['loss'][EPOCHS-1], model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHY4eqoqH9xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_search_hyperparameters(num_tries=10):\n",
        "    losses = np.zeros(num_tries)\n",
        "    settings = []\n",
        "    for i in range(num_tries):\n",
        "        rnn_units = randint(500, 1500)\n",
        "        embedding_dim = randint(200, 400)\n",
        "        BATCH_SIZE = np.random.choice([32,64, 128, 256, 512])\n",
        "        softmax = np.random.choice([True, False])\n",
        "        optimizer = np.random.choice(['adam', 'nadam', 'rmsprop'])\n",
        "        learning_rate = np.random.uniform(0.01, 0.0001)\n",
        "\n",
        "        hidden_layer = np.random.choice([True, False])\n",
        "        hidden_layer_size = randint(100,400)\n",
        "        hidden_layer_activation = np.random.choice(['sigmoid', 'tanh', 'relu', 'elu'])\n",
        "        \n",
        "        if hidden_layer:\n",
        "            hidden_layer = {'size': hidden_layer_size, 'act': hidden_layer_activation}\n",
        "        else:\n",
        "            hidden_layer = None\n",
        "\n",
        "        EPOCHS = 1\n",
        "                \n",
        "        setting = {'rnn_units': rnn_units, 'embedding_dim': embedding_dim, 'BATCH_SIZE': BATCH_SIZE, 'softmax':softmax, 'hidden_layer': hidden_layer, 'optimizer': optimizer, 'learning_rate': learning_rate}\n",
        "        print(setting)\n",
        "        settings.append(setting)\n",
        "        losses[i] = run_model(dataset, setting, 1)\n",
        "    \n",
        "    return losses, settings\n",
        "\n",
        "losses, settings = random_search_hyperparameters()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6YgSQLkvZZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Best found settings\n",
        "settings = [{'rnn_units': 1334, 'embedding_dim': 331, 'BATCH_SIZE': 32, 'softmax': True, 'hidden_layer': None, 'optimizer': 'adam', 'learning_rate': 0.0018792239351562903},\n",
        "            {'rnn_units': 1203, 'embedding_dim': 318, 'BATCH_SIZE': 64, 'softmax': True, 'hidden_layer': None, 'optimizer': 'adam', 'learning_rate': 0.00249201743489572},\n",
        "            {'rnn_units': 1312, 'embedding_dim': 289, 'BATCH_SIZE': 64, 'softmax': True, 'hidden_layer': None, 'optimizer': 'adam', 'learning_rate': 0.0007947486669195956},\n",
        "            {'rnn_units': 1036, 'embedding_dim': 311, 'BATCH_SIZE': 64, 'softmax': True, 'hidden_layer': None, 'optimizer': 'adam', 'learning_rate': 0.0034466245761176593}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHAojPK-sUfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "47e4d0c7-af68-4fa4-f4a6-6a3f4156378d"
      },
      "source": [
        "# Monitor validation loss, for saving the model which generalizes best\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='RNN_model_new.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Best of the above 4 settings\n",
        "s = settings[2]\n",
        "losses, best_model = run_model(dataset, s, 10, 0.75, [checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.4297\n",
            "Epoch 00001: val_loss improved from inf to 1.23736, saving model to RNN_model_new.hdf5\n",
            "3274/3274 [==============================] - 815s 249ms/step - loss: 1.4297 - val_loss: 1.2374\n",
            "Epoch 2/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1840\n",
            "Epoch 00002: val_loss improved from 1.23736 to 1.19505, saving model to RNN_model_new.hdf5\n",
            "3274/3274 [==============================] - 824s 252ms/step - loss: 1.1840 - val_loss: 1.1950\n",
            "Epoch 3/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1510\n",
            "Epoch 00003: val_loss improved from 1.19505 to 1.18378, saving model to RNN_model_new.hdf5\n",
            "3274/3274 [==============================] - 809s 247ms/step - loss: 1.1510 - val_loss: 1.1838\n",
            "Epoch 4/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1353\n",
            "Epoch 00004: val_loss improved from 1.18378 to 1.17852, saving model to RNN_model_new.hdf5\n",
            "3274/3274 [==============================] - 809s 247ms/step - loss: 1.1353 - val_loss: 1.1785\n",
            "Epoch 5/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1279\n",
            "Epoch 00005: val_loss improved from 1.17852 to 1.17456, saving model to RNN_model_new.hdf5\n",
            "3274/3274 [==============================] - 813s 248ms/step - loss: 1.1279 - val_loss: 1.1746\n",
            "Epoch 6/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1240\n",
            "Epoch 00006: val_loss did not improve from 1.17456\n",
            "3274/3274 [==============================] - 827s 253ms/step - loss: 1.1240 - val_loss: 1.1750\n",
            "Epoch 7/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1237\n",
            "Epoch 00007: val_loss did not improve from 1.17456\n",
            "3274/3274 [==============================] - 823s 251ms/step - loss: 1.1237 - val_loss: 1.1769\n",
            "Epoch 8/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1253\n",
            "Epoch 00008: val_loss did not improve from 1.17456\n",
            "3274/3274 [==============================] - 819s 250ms/step - loss: 1.1253 - val_loss: 1.1780\n",
            "Epoch 9/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1287\n",
            "Epoch 00009: val_loss did not improve from 1.17456\n",
            "3274/3274 [==============================] - 817s 249ms/step - loss: 1.1287 - val_loss: 1.1819\n",
            "Epoch 10/10\n",
            "3274/3274 [==============================] - ETA: 0s - loss: 1.1349\n",
            "Epoch 00010: val_loss did not improve from 1.17456\n",
            "3274/3274 [==============================] - 817s 250ms/step - loss: 1.1349 - val_loss: 1.1886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTe-V935iqnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restore model from checkpoint\n",
        "setting = settings[2]\n",
        "best_model = build_model(vocab_size = len(vocab), embedding_dim=setting['embedding_dim'], rnn_units=setting['rnn_units'], batch_size=1, hidden_layer = setting['hidden_layer'], softmax=setting['softmax'])\n",
        "best_model.load_weights('RNN_model_new.hdf5')\n",
        "best_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UJxHN5linxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # generate text using the learned model\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = np.expand_dims(input_eval, 0)\n",
        "  \n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model.predict(input_eval)\n",
        "\n",
        "      # remove the batch dimension      \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = np.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhhVkv9c_1CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_text(best_model, start_string=u\"Climate change is\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}