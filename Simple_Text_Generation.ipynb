{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Text Generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voqx3u6UPj4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBg6L102P_cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
        "    train_data = [line.strip('\\n') for line in file]\n",
        "\n",
        "print('Number of training sentences: ', len(train_data))\n",
        "\n",
        "max_words = 50000 # Max size of the dictionary\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "sequences = tokenizer.texts_to_sequences(train_data)\n",
        "\n",
        "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
        "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
        "text = [item for sublist in sequences for item in sublist]\n",
        "vocab_size = len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmhvlq1kSBe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad1c8077-41bd-4f15-b24b-1178fa35ed72"
      },
      "source": [
        "# Training on 19 words to predict the 20th\n",
        "sentence_len = 20\n",
        "pred_len = 1\n",
        "train_len = sentence_len - pred_len\n",
        "seq = []\n",
        "# Sliding window to generate train data\n",
        "for i in range(len(text)-sentence_len):\n",
        "    seq.append(text[i:i+sentence_len])\n",
        "# Reverse dictionary to decode tokenized sequences back to words\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
        "trainX = []\n",
        "trainy = []\n",
        "for i in seq:\n",
        "    trainX.append(i[:train_len])\n",
        "    trainy.append(i[-1])\n",
        "\n",
        "len(trainy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5117695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMoZ8-dsR-CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model_2 = Sequential([\n",
        "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de7dQjZRVIVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_metric(y_true, y_pred):\n",
        "    return 1- tf.gather(y_pred, tf.dtypes.cast(y_true, tf.int32))\n",
        "\n",
        "# Train model with checkpoints\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd4AGi3_Tj-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"./model_2_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "x = np.asarray(trainX)\n",
        "y = np.asarray(trainy)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv9_u6X9ntrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "8f2ef751-0f23-408d-dd67-c31c179dc70f"
      },
      "source": [
        "model_2.fit(x,y, epochs = 30, batch_size = 512, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 6.0818 - accuracy: 0.1132\n",
            "Epoch 00001: loss improved from inf to 6.08176, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1098s 110ms/step - loss: 6.0818 - accuracy: 0.1132\n",
            "Epoch 2/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.6645 - accuracy: 0.1421\n",
            "Epoch 00002: loss improved from 6.08176 to 5.66452, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1092s 109ms/step - loss: 5.6645 - accuracy: 0.1421\n",
            "Epoch 3/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.5230 - accuracy: 0.1504\n",
            "Epoch 00003: loss improved from 5.66452 to 5.52301, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1108s 111ms/step - loss: 5.5230 - accuracy: 0.1504\n",
            "Epoch 4/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.4331 - accuracy: 0.1561\n",
            "Epoch 00004: loss improved from 5.52301 to 5.43311, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1103s 110ms/step - loss: 5.4331 - accuracy: 0.1561\n",
            "Epoch 5/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.3672 - accuracy: 0.1601\n",
            "Epoch 00005: loss improved from 5.43311 to 5.36716, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1102s 110ms/step - loss: 5.3672 - accuracy: 0.1601\n",
            "Epoch 6/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.3169 - accuracy: 0.1630\n",
            "Epoch 00006: loss improved from 5.36716 to 5.31694, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1094s 109ms/step - loss: 5.3169 - accuracy: 0.1630\n",
            "Epoch 7/30\n",
            "9996/9996 [==============================] - ETA: 0s - loss: 5.2772 - accuracy: 0.1655\n",
            "Epoch 00007: loss improved from 5.31694 to 5.27721, saving model to ./model_2_weights.hdf5\n",
            "9996/9996 [==============================] - 1093s 109ms/step - loss: 5.2772 - accuracy: 0.1655\n",
            "Epoch 8/30\n",
            "5711/9996 [================>.............] - ETA: 7:46 - loss: 5.2312 - accuracy: 0.1677"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqJw2aqR7mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen(model,seq,max_len = 20):\n",
        "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
        "    reaches max_len'''\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
        "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = tf.keras.preprocessing.sequence.pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "        \n",
        "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYe4ST02daPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen(model_2,'The climate change is a global crisis for ')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}