{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('pytorch using ', device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "max_words = 6000\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "max_samples = 1000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
    "    train_data = [line.strip('\\n') for line in file]\n",
    "\n",
    "# tokenize words in text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "# sentences as word sequences with max_words or words in text if less\n",
    "sequences = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "# Flatten the list of lists to apply sliding window\n",
    "text = [item for sublist in sequences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sliding window to generate train data\n",
    "seq = []\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "\n",
    "# reduce length of seq for performance reasons and of cause test_set\n",
    "seq = seq[:max_samples]\n",
    "\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# set data, label\n",
    "X = []\n",
    "y = []\n",
    "for i in seq:\n",
    "    X.append(i[:train_len])\n",
    "    y.append(i[-1])\n",
    "\n",
    "num_samples = len(y)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "words_in_text = len(tokenizer.word_index)\n",
    "vocab_size = words_in_text if words_in_text < max_words else max_words\n",
    "\n",
    "print('Number of training sentences: ', len(train_data))\n",
    "print('Number of training samples: ', num_samples)\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# split data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "# setting type for device\n",
    "X_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "y_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "\n",
    "# create feature and targets tensor for train set.\n",
    "torch_X_train = torch.from_numpy(X_train).type(X_type)\n",
    "torch_y_train = torch.from_numpy(y_train).type(y_type)\n",
    "\n",
    "# create feature and targets tensor for valid set.\n",
    "torch_X_valid = torch.from_numpy(X_valid).type(X_type)\n",
    "torch_y_valid = torch.from_numpy(y_valid).type(y_type)\n",
    "\n",
    "# Pytorch train and valid sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)\n",
    "valid = torch.utils.data.TensorDataset(torch_X_valid, torch_y_valid)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print('batches: ', len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_f, optimizer, data_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    num_batches = len(data_loader) \n",
    "    \n",
    "    for X_batch, y_batch in data_loader:\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_f(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(pred.data, 1)[1] \n",
    "        correct += (predicted == y_batch).sum()\n",
    "        batch_losses += loss.item()\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses / float(batch_size * num_batches)\n",
    "    accu = correct / float(batch_size * num_batches)\n",
    "    \n",
    "    return loss, accu.cpu()\n",
    "\n",
    "\n",
    "def test(model, loss_f, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    num_batches = len(data_loader) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            \n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = loss_f(pred, y_batch)\n",
    "            \n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(pred.data, 1)[1] \n",
    "            correct += (predicted == y_batch).sum()\n",
    "            batch_losses += loss.item()\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses  / float(batch_size * num_batches)\n",
    "    accu = correct / float(batch_size * num_batches)\n",
    "    \n",
    "    return loss, accu.cpu()\n",
    "\n",
    "\n",
    "def fit(model, train_loader, valid_loader, optimizer, loss_f, scheduler=None):\n",
    "    print(\"fitting model...\")\n",
    "    \n",
    "    train_eval = []\n",
    "    valid_eval = []\n",
    "    fit_start = time.time()\n",
    "\n",
    "    #iteration 0 (only testing initial model)\n",
    "    train_loss, train_accu = test(model, loss_f, train_loader)\n",
    "    train_eval.append( (train_loss, train_accu) )\n",
    "    valid_loss, valid_accu = test(model, loss_f, valid_loader)\n",
    "    valid_eval.append( (valid_loss, valid_accu) )\n",
    "    print('Epoch: 0')\n",
    "    print('Train:        Loss: {:.6f}   Accuracy: {:.2f}%  '.format(train_loss, train_accu * 100))\n",
    "    print('Validation:   Loss: {:.6f}   Accuracy: {:.2f}%\\n'.format(valid_loss , valid_accu * 100))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_accu = train(model, loss_f, optimizer, train_loader)\n",
    "        train_eval.append( (train_loss, train_accu) )\n",
    "        \n",
    "        valid_loss, valid_accu = test(model, loss_f, valid_loader)\n",
    "        valid_eval.append( (valid_loss, valid_accu) )\n",
    "\n",
    "        if(scheduler != None): scheduler.step()  # update learning rate\n",
    "\n",
    "        print('Epoch: {}, \\t duration: {:.2f}sec'.format(epoch + 1, time.time() - epoch_start))\n",
    "        print('Train:        Loss: {:.6f}   Accuracy: {:.2f}%  '.format(train_loss, train_accu * 100))\n",
    "        print('Validation:   Loss: {:.6f}   Accuracy: {:.2f}%\\n'.format(valid_loss , valid_accu * 100))\n",
    "\n",
    "    d = time.gmtime(time.time() - fit_start)\n",
    "    print('duration of fitting: {:2d}h {:2d}min {:2d}sec'.format(d.tm_hour, d.tm_min, d.tm_sec))\n",
    "\n",
    "    # save model\n",
    "    path = './models/' + model.__class__.__name__\n",
    "    torch.save(model, path)\n",
    "\n",
    "    return np.array(train_eval), np.array(valid_eval)\n",
    "\n",
    "\n",
    "def plot_eval(train_eval, valid_eval):\n",
    "\n",
    "    train_losses = train_eval[:, 0]\n",
    "    train_accus  = train_eval[:, 1]\n",
    "    valid_losses = valid_eval[:, 0]\n",
    "    valid_accus  = valid_eval[:, 1]\n",
    "    min_loss_idx = np.argmin(valid_losses)\n",
    "    max_accu_idx = np.argmax(valid_accus)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(valid_losses, label=\"validation\")\n",
    "    plt.scatter(min_loss_idx, valid_losses[min_loss_idx], c='r', label=\"minimum\")\n",
    "    plt.title(\"evaluation of losses\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(train_accus, label=\"train\")\n",
    "    plt.plot(valid_accus, label=\"validation\")\n",
    "    plt.scatter(max_accu_idx, valid_accus[max_accu_idx], c='r', label=\"maximum\")\n",
    "    plt.title(\"evaluation of accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "m1_embedding_dim=50\n",
    "m1_lstm_hidden = 100\n",
    "m1_dense_features = 100\n",
    "m1_dropout = 0.3\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.embed = nn.Embedding(  num_embeddings=vocab_size + 1, \n",
    "                                    embedding_dim=m1_embedding_dim, \n",
    "                                    padding_idx=train_len)\n",
    "        self.rnn = nn.LSTM(input_size=50, \n",
    "                            hidden_size=m1_lstm_hidden, \n",
    "                            num_layers=2)\n",
    "        self.dense1 = nn.Linear(in_features=m1_lstm_hidden, \n",
    "                                out_features=m1_dense_features)\n",
    "        self.dense2 = nn.Linear(in_features=m1_dense_features, \n",
    "                                out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.rnn(x)  # _ = (h, c)\n",
    "        x = x[:,-1,:]  # get last step of output\n",
    "        x = F.relu( self.dense1(x) )\n",
    "        x = F.dropout(x, p=m1_dropout)\n",
    "        x = self.dense2(x)\n",
    "        # x = F.log_softmax(x)  # this step is part of CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "\n",
    "model1 = Model1().to(device)\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "m2_embedding_dim=50\n",
    "m2_gru_hidden = 100\n",
    "m2_dense_features = 100\n",
    "m2_dropout = 0.3\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.embed = nn.Embedding(  num_embeddings=vocab_size + 1, \n",
    "                                    embedding_dim=m2_embedding_dim, \n",
    "                                    padding_idx=train_len)\n",
    "        self.rnn = nn.GRU(input_size=50, \n",
    "                            hidden_size=m2_gru_hidden, \n",
    "                            num_layers=3)\n",
    "        self.dense1 = nn.Linear(in_features=m2_gru_hidden, \n",
    "                                out_features=m2_dense_features)\n",
    "        self.dense2 = nn.Linear(in_features=m2_dense_features, \n",
    "                                out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.rnn(x)  # _ = (h, c)\n",
    "        x = x[:,-1,:]  # get last step of output\n",
    "        x = F.relu( self.dense1(x) )\n",
    "        x = F.dropout(x, p=m1_dropout)\n",
    "        x = self.dense2(x)\n",
    "        # x = F.log_softmax(x)  # this step is part of CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "\n",
    "model2 = Model2().to(device)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "m3_embedding_dim=50\n",
    "m3_lstm_hidden = 100\n",
    "m3_dense_features = 100\n",
    "m3_dropout = 0.3\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.embed = nn.Embedding(  num_embeddings=vocab_size + 1, \n",
    "                                    embedding_dim=m3_embedding_dim, \n",
    "                                    padding_idx=train_len)\n",
    "        self.rnn = nn.LSTM(input_size=50, \n",
    "                            hidden_size=m3_lstm_hidden, \n",
    "                            num_layers=1)\n",
    "        self.dense1 = nn.Linear(in_features=m3_lstm_hidden, \n",
    "                                out_features=m3_dense_features)\n",
    "        self.dense2 = nn.Linear(in_features=m3_dense_features, \n",
    "                                out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.rnn(x)  # _ = (h, c)\n",
    "        x = x[:,-1,:]  # get last step of output\n",
    "        x = F.relu( self.dense1(x) )\n",
    "        x = F.dropout(x, p=m1_dropout)\n",
    "        x = self.dense2(x)\n",
    "        # x = F.log_softmax(x)  # this step is part of CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "\n",
    "model3 = Model3().to(device)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "scheduler3 = optim.lr_scheduler.MultiplicativeLR(optimizer3, lambda epoch: 0.9)\n",
    "\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_train1, eval_valid1 = fit(model1, train_loader, valid_loader, optimizer1, loss_f)\n",
    "plot_eval(eval_train1, eval_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_train2, eval_valid2 = fit(model2, train_loader, valid_loader, optimizer2, loss_f)\n",
    "plot_eval(eval_train2, eval_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_train3, eval_valid3 = fit(model3, train_loader, valid_loader, optimizer3, loss_f, scheduler3)\n",
    "plot_eval(eval_train3, eval_valid3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, sentence, iterations=10):\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    seq = [item for sublist in sequences for item in sublist] # tokenized words\n",
    "    if(len(seq) < train_len): return -1\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        iput = np.array(seq[-19:])\n",
    "        iput = np.vstack(iput).T\n",
    "        iput = torch.from_numpy(iput).type(X_type)\n",
    "\n",
    "        # oput = F.log_softmax( model(iput) )  # prob dist\n",
    "        oput = model(iput)\n",
    "        \n",
    "        oput = oput.cpu().detach().numpy()[0]\n",
    "        # print(np.average(oput))\n",
    "        oput = np.argmax(oput) # one-hot to long\n",
    "        seq.append(oput)\n",
    "\n",
    "    pred_sen = ''\n",
    "    for tk in seq:\n",
    "        pred_sen += reverse_word_map[tk] + ' '\n",
    "    return pred_sen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sen = \"Calls on Member States, in the light of the growing risks which the European Union is running in terms of energy security\"\n",
    "gen(model1, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594898453993",
   "display_name": "Python 3.7.7 64-bit ('tensorflow-envi': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}