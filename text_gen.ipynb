{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'cuda'"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "vocab_size = 50000\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of training sentences:  2477\n"
    }
   ],
   "source": [
    "# data retrieval \n",
    "\n",
    "X, y = None, None\n",
    "\n",
    "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
    "    data = [line.strip('\\n') for line in file]\n",
    "\n",
    "print('Number of training sentences: ', len(data))\n",
    "\n",
    "#TODO text to sequence, \n",
    "#TODO sequence to one hots, \n",
    "#TODO one hot to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(X, y, test_size=.3, train_size=None):\n",
    "    \n",
    "    random_state = check_random_state(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    # setting type for device\n",
    "    X_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "    y_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "\n",
    "    # create feature and targets tensor for train set.\n",
    "    torch_X_train = torch.from_numpy(X_train).type(X_type)\n",
    "    torch_y_train = torch.from_numpy(y_train).type(y_type)\n",
    "\n",
    "    # create feature and targets tensor for test set.\n",
    "    torch_X_test = torch.from_numpy(X_test).type(X_type)\n",
    "    torch_y_test = torch.from_numpy(y_test).type(y_type)\n",
    "\n",
    "    # Pytorch train and test sets\n",
    "    train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)\n",
    "    test = torch.utils.data.TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "    # data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(model, loss_f, optimizer, data_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_f(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(pred.data, 1)[1] \n",
    "        correct += (predicted == y_batch).sum()\n",
    "        batch_losses += loss.item()\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses  / float(BATCH_SIZE*len(data_loader))\n",
    "    accu = correct * 100 / float(BATCH_SIZE*len(data_loader))\n",
    "    \n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def test(model, loss_f, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            \n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = loss_f(pred, y_batch)\n",
    "            \n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(pred.data, 1)[1] \n",
    "            correct += (predicted == y_batch).sum()\n",
    "            batch_losses += loss.item()\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses  / float(BATCH_SIZE*len(data_loader))\n",
    "    accu = correct * 100 / float(BATCH_SIZE*len(data_loader))\n",
    "    \n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def fit(model, train_loader, test_loader, optimizer, loss_f, batch_size=32, epochs=5):\n",
    "    \n",
    "    train_eval = []\n",
    "    test_eval = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss, train_accu = train(model, loss_f, optimizer, train_loader)\n",
    "        train_eval.append( (train_loss, train_accu) )\n",
    "        \n",
    "        test_loss, test_accu = test(model, loss_f, test_loder)\n",
    "        test_eval.append( (test_loss, test_accu) )\n",
    "\n",
    "        print('Epoch: {}'.format(epoch + 1))\n",
    "        print('Train:  Loss: {:.6f}   Accuracy: {:.2f}%  '.format(train_loss, train_accu))\n",
    "        print('Test:   Loss: {:.6f}   Accuracy: {:.2f}%\\n'.format(test_loss , test_accu ))\n",
    "\n",
    "    return np.array(train_eval), np.array(test_eval)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, train_eval=None, test_eval=None):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        pred = model(test_imgs)\n",
    "        predicted = torch.max(pred,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "\n",
    "    test_accu = float(correct*100) / float(BATCH_SIZE*len(test_loader))\n",
    "    print(\"Test accuracy: {:.3f}% \".format( test_accu ))\n",
    "\n",
    "    if(train_eval is None or test_eval is None): return\n",
    "\n",
    "    train_losses = train_eval[:, 0]\n",
    "    train_accus  = train_eval[:, 1]\n",
    "    test_losses  =  test_eval[:, 0]\n",
    "    test_accus   =  test_eval[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(test_losses, label=\"test\")\n",
    "    plt.title(\"evaluation of losses\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(train_accus, label=\"train\")\n",
    "    plt.plot(test_accus, label=\"test\")\n",
    "    plt.title(\"evaluation of accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=50, padding_idx=train_len)\n",
    "        self.lstm = nn.LSTM(input_size=50, hidden_size=100, num_layers=2) #seq2seq\n",
    "        self.dense1 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.dense2 = nn.Linear(in_features=100, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.lstm(x)\n",
    "        x = F.relu( self.dense(x) )\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.log_softmax( self.dense2(x) )\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'NoneType'>",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-dab641441f2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextGenModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-08a77efd4e64>\u001b[0m in \u001b[0;36mdata_loader\u001b[1;34m(X, y, test_size, train_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# setting type for device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stdenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2127\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2129\u001b[1;33m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2130\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m   2131\u001b[0m                                               default_test_size=0.25)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stdenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, test_loader = data_loader(X, y)\n",
    "\n",
    "model = TextGenModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_f = nn.CrossEntropyLoss.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-4c92f02ad79a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, test_loader, optimizer, loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitstdenvcondae23e8eca656b44b38edc98edd8c0a60b",
   "display_name": "Python 3.8.3 64-bit ('stdenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}