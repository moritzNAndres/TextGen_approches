{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'cuda'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "max_vocab_size = 50000\n",
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of training sentences:  2477\nvocab size: 62237\n"
    }
   ],
   "source": [
    "# data retrieval \n",
    "\n",
    "with open('processed_texts.csv', 'r', encoding='UTF-8') as file:\n",
    "    train_data = [line.strip('\\n') for line in file]\n",
    "\n",
    "print('Number of training sentences: ', len(train_data))\n",
    "\n",
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "sequences = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "# Sliding window to generate train data\n",
    "seq = []\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Each row in seq is a 20 word long window. We append the first 19 words as the input to predict the 20th word\n",
    "X = []\n",
    "y = []\n",
    "for i in seq:\n",
    "    X.append(i[:train_len])\n",
    "    y.append(i[-1])\n",
    "\n",
    "print(\"vocab size:\", vocab_size)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(X, y, test_size=.3, train_size=None):\n",
    "    \n",
    "    random_state = check_random_state(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    # setting type for device\n",
    "    X_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "    y_type = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "\n",
    "    # create feature and targets tensor for train set.\n",
    "    torch_X_train = torch.from_numpy(X_train).type(X_type)\n",
    "    torch_y_train = torch.from_numpy(y_train).type(y_type)\n",
    "\n",
    "    # create feature and targets tensor for test set.\n",
    "    torch_X_test = torch.from_numpy(X_test).type(X_type)\n",
    "    torch_y_test = torch.from_numpy(y_test).type(y_type)\n",
    "\n",
    "    # Pytorch train and test sets\n",
    "    train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)\n",
    "    test = torch.utils.data.TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "    # data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = data_loader(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_f, optimizer, data_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    num_batches = len(data_loader) \n",
    "    i = 0\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_f(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Total correct predictions\n",
    "        predicted = torch.max(pred.data, 1)[1] \n",
    "        correct += (predicted == y_batch).sum()\n",
    "        batch_losses += loss.item()\n",
    "        # i+=1\n",
    "        # print(\"{} of {} batches\".format(i, num_batches))\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses  / float(batch_size * num_batches)\n",
    "    accu = correct * 100 / float(batch_size * num_batches)\n",
    "    \n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def test(model, loss_f, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    batch_losses = 0\n",
    "    num_batches = len(data_loader) \n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            \n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            loss = loss_f(pred, y_batch)\n",
    "            \n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(pred.data, 1)[1] \n",
    "            correct += (predicted == y_batch).sum()\n",
    "            batch_losses += loss.item()\n",
    "            i+=1\n",
    "            print(\"{}\\tof\\t{}\\tbatches\".format(i, num_batches))\n",
    "    \n",
    "    # average loss and accuracies per epoch\n",
    "    loss = batch_losses  / float(batch_size * num_batches)\n",
    "    accu = correct * 100 / float(batch_size * num_batches)\n",
    "    \n",
    "    return loss, accu\n",
    "\n",
    "\n",
    "def fit(model, train_loader, test_loader, optimizer, loss_f):\n",
    "    \n",
    "    train_eval = []\n",
    "    test_eval = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss, train_accu = train(model, loss_f, optimizer, train_loader)\n",
    "        train_eval.append( (train_loss, train_accu) )\n",
    "        \n",
    "        test_loss, test_accu = test(model, loss_f, test_loder)\n",
    "        test_eval.append( (test_loss, test_accu) )\n",
    "\n",
    "        print('Epoch: {}'.format(epoch + 1))\n",
    "        print('Train:  Loss: {:.6f}   Accuracy: {:.2f}%  '.format(train_loss, train_accu))\n",
    "        print('Test:   Loss: {:.6f}   Accuracy: {:.2f}%\\n'.format(test_loss , test_accu ))\n",
    "\n",
    "    return np.array(train_eval), np.array(test_eval)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, train_eval=None, test_eval=None):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        pred = model(test_imgs)\n",
    "        predicted = torch.max(pred,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "\n",
    "    test_accu = float(correct*100) / float(batch_size*len(test_loader))\n",
    "    print(\"Test accuracy: {:.3f}% \".format( test_accu ))\n",
    "\n",
    "    if(train_eval is None or test_eval is None): return\n",
    "\n",
    "    train_losses = train_eval[:, 0]\n",
    "    train_accus  = train_eval[:, 1]\n",
    "    test_losses  =  test_eval[:, 0]\n",
    "    test_accus   =  test_eval[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(test_losses, label=\"test\")\n",
    "    plt.title(\"evaluation of losses\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(train_accus, label=\"train\")\n",
    "    plt.plot(test_accus, label=\"test\")\n",
    "    plt.title(\"evaluation of accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextGenModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=50, padding_idx=train_len)\n",
    "        # self.lstm = nn.LSTM(input_size=50, hidden_size=100, num_layers=2, batch_first=True) #seq2seq\n",
    "        self.lstm = nn.LSTM(50, 100, 2)\n",
    "        self.dense1 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.dense2 = nn.Linear(in_features=100, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        lstm_output, _ = self.lstm(x)  # _ = (h, c)\n",
    "        x = lstm_output[:,-1,:]  # get last step of output\n",
    "        x = F.relu( self.dense1(x) )\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.log_softmax( self.dense2(x) )\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TextGenModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_f = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4c92f02ad79a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-5854d798fe3d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, train_loader, test_loader, optimizer, loss_f)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mtrain_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accu\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-5854d798fe3d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_f, optimizer, data_loader)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-envi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-8e79d1883d3d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlstm_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# _ = (h, c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# get last step of output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-envi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-envi\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 570\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, test_loader, optimizer, loss_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Done."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# updating output (unrelated code)\n",
    "\n",
    "import IPython\n",
    "out = display(IPython.display.Pretty('Starting'), display_id=True)\n",
    "time.sleep(1)\n",
    "\n",
    "for i in range(8):\n",
    "  out.update(IPython.display.Pretty('Going' + '.' * (i % 3 + 1)))\n",
    "  time.sleep(0.5)\n",
    "\n",
    "out.update(IPython.display.Pretty('Done.'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594878320878",
   "display_name": "Python 3.7.7 64-bit ('tensorflow-envi': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}